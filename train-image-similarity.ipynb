{"cells":[{"cell_type":"markdown","metadata":{"id":"eufgAL3xy6Zm"},"source":["# Train Image Similarity"]},{"cell_type":"markdown","metadata":{"id":"QqCQpjnQz9CI"},"source":["## Mount drive etc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StLnj_Q_y3vy","trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"_8hofRqTz-5J"},"source":["## Run the Training Script"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"bKUAY2Z790q_","trusted":true},"outputs":[],"source":["import torch\n","from PIL import Image\n","import os\n","from tqdm import tqdm\n","from torch.utils.data import Dataset\n","import matplotlib\n","matplotlib.use(\"TkAgg\")\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as T\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","%matplotlib inline\n","import os\n","import json    \n","os.environ['KMP_DUPLICATE_LIB_OK']='True'"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"Vt3SI_V69yuG","trusted":true},"outputs":[],"source":["class FolderDataset(Dataset):\n","    def __init__(self, main_dir, transform=None):\n","        self.main_dir = main_dir\n","        self.transform = transform\n","        self.all_folders = os.listdir(main_dir)\n","        self.all_imgs=[]\n","        for folder in self.all_folders:\n","            folder_dir=os.path.join(main_dir, folder)\n","            files=os.listdir(folder_dir)\n","            for file in files:\n","                self.all_imgs.append(os.path.join(folder_dir, file))\n","        with open(\"geological_map.json\", 'w', encoding='utf-8') as f:\n","            json.dump(self.all_imgs, f)\n","\n","    def __len__(self):\n","        return len(self.all_imgs)\n","\n","    def __getitem__(self, idx):\n","        img_loc = self.all_imgs[idx]\n","        image = Image.open(img_loc).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            tensor_image = self.transform(image)\n","        # print(tensor_image.shape)\n","        return tensor_image, tensor_image\n"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"TfTyqEPL9ttJ","trusted":true},"outputs":[],"source":["\n","class ConvEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # self.img_size = img_size\n","        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.maxpool1 = nn.MaxPool2d((2, 2))\n","\n","        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.maxpool2 = nn.MaxPool2d((2, 2))\n","\n","        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n","        self.relu3 = nn.ReLU(inplace=True)\n","        self.maxpool3 = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, x):\n","        # Downscale the image with conv maxpool etc.\n","        # print(x.shape)\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","        x = self.maxpool1(x)\n","\n","        # print(x.shape)\n","\n","        x = self.conv2(x)\n","        x = self.relu2(x)\n","        x = self.maxpool2(x)\n","\n","        # print(x.shape)\n","\n","        x = self.conv3(x)\n","        x = self.relu3(x)\n","        x = self.maxpool3(x)\n","\n","        # print(x.shape)\n","        return x\n","\n","\n","class ConvDecoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.deconv1 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n","        # self.upsamp1 = nn.UpsamplingBilinear2d(2)\n","        self.relu1 = nn.ReLU(inplace=True)\n","\n","        self.deconv2 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n","#         self.upsamp1 = nn.UpsamplingBilinear2d(2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","\n","        self.deconv3 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n","#         self.upsamp1 = nn.UpsamplingBilinear2d(2)\n","        self.relu3 = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        # print(x.shape)\n","        x = self.deconv1(x)\n","        x = self.relu1(x)\n","        # print(x.shape)\n","\n","        x = self.deconv2(x)\n","        x = self.relu2(x)\n","        # print(x.shape)\n","\n","        x = self.deconv3(x)\n","        x = self.relu3(x)\n","        # print(x.shape)\n","        return x\n"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"IjLkAZy49Z7v","trusted":true},"outputs":[],"source":["IMG_PATH = \"geological_similarity\"\n","IMG_HEIGHT = 512  # The images are already resized here\n","IMG_WIDTH = 512  # The images are already resized here\n","\n","SEED = 42\n","TRAIN_RATIO = 0.75\n","VAL_RATIO = 1 - TRAIN_RATIO\n","SHUFFLE_BUFFER_SIZE = 100\n","\n","LEARNING_RATE = 1e-3\n","EPOCHS = 2\n","TRAIN_BATCH_SIZE = 32  # Let's see, I don't have GPU, Google Colab is best hope\n","TEST_BATCH_SIZE = 32  # Let's see, I don't have GPU, Google Colab is best hope\n","FULL_BATCH_SIZE = 32\n","\n","AUTOENCODER_MODEL_PATH = \"baseline_autoencoder.pt\"\n","ENCODER_MODEL_PATH = \"geological_encoding.pt\"\n","DECODER_MODEL_PATH = \"geological_decoding.pt\"\n","EMBEDDING_SHAPE = (1, 64, 64, 64)\n","# TEST_RATIO = 0.2\n"]},{"cell_type":"code","execution_count":89,"metadata":{"id":"Vn7OetpD4gjZ","trusted":true},"outputs":[],"source":["\"\"\"\n","I can write this if we need custom training loop etc.\n","I usually use this in PyTorch.\n","\"\"\"\n","\n","__all__ = [\"train_step\", \"val_step\", \"create_embedding\"]\n","\n","import torch\n","import torch.nn as nn\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):\n","    # device = \"cuda\"\n","    encoder.train()\n","    decoder.train()\n","\n","    # print(device)\n","\n","    for batch_idx, (train_img, target_img) in enumerate(train_loader):\n","        train_img = train_img.to(device)\n","        target_img = target_img.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        enc_output = encoder(train_img)\n","        dec_output = decoder(enc_output)\n","        loss = loss_fn(dec_output, target_img)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","    return loss.item()\n","\n","\n","def val_step(encoder, decoder, val_loader, loss_fn, device):\n","    encoder.eval()\n","    decoder.eval()\n","\n","    with torch.no_grad():\n","        for batch_idx, (train_img, target_img) in enumerate(val_loader):\n","            train_img = train_img.to(device)\n","            target_img = target_img.to(device)\n","\n","            enc_output = encoder(train_img)\n","            dec_output = decoder(enc_output)\n","\n","            loss = loss_fn(dec_output, target_img)\n","\n","    return loss.item()"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"DXPDCa8a9R3c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["------------ Creating Dataset ------------\n","------------ Dataset Created ------------\n","------------ Creating DataLoader ------------\n","29998\n"]}],"source":["if torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","\n","# print(\"Setting Seed for the run, seed = {}\".format(config.SEED))\n","\n","# seed_everything(config.SEED)\n","\n","transforms = T.Compose([T.ToTensor(), T.CenterCrop(24)])\n","print(\"------------ Creating Dataset ------------\")\n","full_dataset = FolderDataset(IMG_PATH, transforms)\n","\n","train_size = int(TRAIN_RATIO * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","\n","train_dataset, val_dataset = torch.utils.data.random_split(\n","    full_dataset, [train_size, val_size]\n",")\n","\n","print(\"------------ Dataset Created ------------\")\n","print(\"------------ Creating DataLoader ------------\")\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True\n",")\n","val_loader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=TEST_BATCH_SIZE\n",")\n","\n","full_loader = torch.utils.data.DataLoader(\n","    full_dataset, batch_size=FULL_BATCH_SIZE\n",")\n","print(len(full_dataset.all_imgs))"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["------------ Dataloader Cretead ------------\n","Moving models to CPU\n","------------ Training started ------------\n","Epochs = 0, Training Loss : 0.007516584824770689\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 1/2 [00:24<00:24, 24.73s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss decreased, saving new best model\n","Epochs = 0, Validation Loss : 0.004617605824023485\n","Epochs = 1, Training Loss : 0.00487650278955698\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:48<00:00, 24.09s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss decreased, saving new best model\n","Epochs = 1, Validation Loss : 0.0026516627985984087\n","Training Done\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["print(\"------------ Dataloader Cretead ------------\")\n","\n","# print(train_loader)\n","loss_fn = nn.MSELoss()\n","\n","encoder = ConvEncoder()\n","decoder = ConvDecoder()\n","\n","if torch.cuda.is_available():\n","    print(\"GPU Availaible moving models to GPU\")\n","else:\n","    print(\"Moving models to CPU\")\n","\n","encoder.to(device)\n","decoder.to(device)\n","\n","# print(device)\n","\n","autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n","optimizer = optim.AdamW(autoencoder_params, lr=LEARNING_RATE)\n","\n","# early_stopper = utils.EarlyStopping(patience=5, verbose=True, path=)\n","max_loss = 9999\n","\n","print(\"------------ Training started ------------\")\n","\n","for epoch in tqdm(range(EPOCHS)):\n","    train_loss = train_step(\n","        encoder, decoder, train_loader, loss_fn, optimizer, device=device\n","    )\n","    print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n","    val_loss = val_step(\n","        encoder, decoder, val_loader, loss_fn, device=device\n","    )\n","\n","    # Simple Best Model saving\n","    if val_loss < max_loss:\n","        print(\"Validation Loss decreased, saving new best model\")\n","        torch.save(encoder.state_dict(), ENCODER_MODEL_PATH)\n","        torch.save(decoder.state_dict(), DECODER_MODEL_PATH)\n","\n","    print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n","\n","print(\"Training Done\")"]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":true},"outputs":[],"source":["embedding_dim = (32,64, 3, 3)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["29998\n"]}],"source":["print(len(full_dataset.all_imgs))"]},{"cell_type":"code","execution_count":94,"metadata":{"trusted":true},"outputs":[],"source":["def create_embedding(encoder, full_loader, embedding_dim, device):\n","    encoder.eval()\n","    embedding = None\n","    # print(embedding.shape)\n","\n","    with torch.no_grad():\n","        for batch_idx, (train_img, target_img) in enumerate(full_loader):\n","            train_img = train_img.to(device)\n","            enc_output = encoder(train_img).cpu()\n","            print(enc_output.shape)\n","            if embedding==None:\n","                embedding=enc_output\n","            else:\n","                embedding = torch.cat((embedding, enc_output), 0)\n","            # print(embedding.shape)\n","    \n","    return embedding\n"]},{"cell_type":"code","execution_count":95,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 64, 3, 3])\n"]},{"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'shape'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-95-a7b33251bbef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-94-b279667ae5ac>\u001b[0m in \u001b[0;36mcreate_embedding\u001b[1;34m(encoder, full_loader, embedding_dim, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0menc_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0membedding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}],"source":["embedding = create_embedding(encoder, full_loader, embedding_dim, device)"]},{"cell_type":"code","execution_count":58,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([30030, 64, 3, 3])\n"]}],"source":["print(embedding.shape)"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[],"source":["# embedding2 = embedding[4700:, :, :, ]"]},{"cell_type":"code","execution_count":60,"metadata":{"trusted":true},"outputs":[],"source":["# print(embedding2.shape)"]},{"cell_type":"code","execution_count":61,"metadata":{"trusted":true},"outputs":[],"source":["numpy_embedding = embedding.cpu().detach().numpy()"]},{"cell_type":"code","execution_count":62,"metadata":{"trusted":true},"outputs":[],"source":["# numpy_embedding = embedding2.cpu().detach().numpy()"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(30030, 64, 3, 3)\n"]}],"source":["print(numpy_embedding.shape)"]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[],"source":["num_images = numpy_embedding.shape[0]\n","# print(num_images)"]},{"cell_type":"code","execution_count":65,"metadata":{"trusted":true},"outputs":[],"source":["flattened_embedding = numpy_embedding.reshape((num_images, -1))"]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(30030, 576)\n"]}],"source":["print(flattened_embedding.shape)"]},{"cell_type":"code","execution_count":67,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":68,"metadata":{"trusted":true},"outputs":[],"source":["np.save(\"geological_embed.npy\", flattened_embedding)"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["# np.save(\"../input/animals-data/data_embedding.npy\", flattened_embedding)"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["flattend_embedding_reloaded = np.load(\"data_embedding_f.npy\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["encoder=ConvEncoder()\n","encoder.load_state_dict(torch.load('baseline_encoder.pt'))\n","decoder=ConvDecoder()\n","decoder.load_state_dict(torch.load('baseline_decoder.pt'))"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1728])\n","tensor(0.9979)\n","<class 'torch.Tensor'>\n"]}],"source":["\n","encoder.eval()\n","embedding = torch.randn(embedding_dim)\n","# print(embedding.shape)\n","\n","with torch.no_grad():\n","    for batch_idx, (train_img, target_img) in enumerate(full_loader):\n","        train_img = train_img.to(device)\n","        #print(train_img==target_img)\n","        # print(train_img.shape)\n","        \n","        enc_output = encoder(train_img).cpu()\n","        dec_output= decoder(enc_output).cpu()\n","        # print(enc_output.shape)\n","        # print(target_img.shape)\n","        # print(dec_output.shape)\n","\n","        output_ten=torch.flatten(dec_output[0])\n","        print(output_ten.shape)\n","        target_ten=torch.flatten(target_img[0])\n","        cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n","        similarity = cos(output_ten,target_ten)\n","        print(similarity)\n","        print(type(similarity))\n","        trans=T.ToPILImage()\n","        break\n","        # embedding = torch.cat((embedding, enc_output), 0)\n","        # print(embedding.shape)\n","\n","\n","#         break\n"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x1d3e62c38b0>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.imshow(trans(target_img[0]))\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x1d3e6320130>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.imshow(trans(dec_output[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
